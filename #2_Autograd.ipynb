{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d47d5999",
   "metadata": {},
   "source": [
    "# <b>Autograd</b>\n",
    "\n",
    "In this section we talk about automatic differentiation calculating with tensors. <br>\n",
    " <b>Why do we need autograd?</b><br>\n",
    " We need to differentiation on backward propagation's step for our loss function and optimizing tensor computing process. In the deep learning training loops, we use this computing and chain-rule differentiation. <br>Pytorch is help us on this with <b>'autograd'.</b> <br> <br> \n",
    "We will look closely at <b>requires_grad, backward, chain-rule (Differentation), Custom Forward and Backward Operations</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cb253e",
   "metadata": {},
   "source": [
    "### <i>\"Creating Tensor is the first step of Backward Propagation\"</i>\n",
    "\n",
    "Why do we say like this because when we create tensors, we specify whether they will be included in gradient calculations or not with the help of requires_grad property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11144812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3244037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(3.0, requires_grad =True) # creating tensor with basic torch operations\n",
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f006bcbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5700, 0.8600, 0.9500], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.tensor( [0.57, 0.86, 0.95],requires_grad=True)\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec7e50",
   "metadata": {},
   "source": [
    "### Basic Computation Graph and Backward Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e17767",
   "metadata": {},
   "source": [
    "The <code>backward()</code> method can only be used directly when the output tensor is a scalar.<br>\n",
    "If we want to compute gradients for vector-valued tensors, PyTorch‚Äôs autograd does not automatically know how to handle them. In such cases, we must either provide a <b>gradient argument</b> to <code>backward()</code> or use the <b>Jacobian matrix</b> for vector, matrix (2D), or higher-dimensional outputs.<br><br>\n",
    "\n",
    "<b>&lt;&lt;&lt; WARNING &gt;&gt;&gt;</b><br>\n",
    "If we create a tensor like this and try to use autograd:<br><br>\n",
    "\n",
    "<pre>\n",
    "<code>\n",
    "x = torch.tensor([0.5, 0.8, 0.95], requires_grad=True)\n",
    "y = x**3 * 2 + 4\n",
    "y.backward()\n",
    "x.grad\n",
    "</code>\n",
    "</pre>\n",
    "\n",
    "the output will be something like:<br>\n",
    "<i>RuntimeError: grad can be implicitly created only for scalar outputs</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b56f7ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 57.0\n",
      "dy/dx = 26.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "y = 3 * x**2 + 2 * x + 1\n",
    "y.backward()  # work clearly because output tensor is scalar (y)\n",
    "\n",
    "print(\"y =\", y.item())\n",
    "print(\"dy/dx =\", x.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6fe87d",
   "metadata": {},
   "source": [
    "Example for chain-rule;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b99f10cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 2.0\n",
      "a: 4.0\n",
      "b: 17.0\n",
      "c: -0.9613974690437317\n",
      "d: -1.9227949380874634\n",
      "Gradient dd/dx: -6.6039204597473145\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "a = x**2       \n",
    "b = 3*a + 5     \n",
    "c = torch.sin(b) \n",
    "d = 2 * c  \n",
    "\n",
    "d.backward()\n",
    "\n",
    "print(f\"x: {x.item()}\")\n",
    "print(f\"a: {a.item()}\")\n",
    "print(f\"b: {b.item()}\")\n",
    "print(f\"c: {c.item()}\")\n",
    "print(f\"d: {d.item()}\")\n",
    "print(f\"Gradient dd/dx: {x.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10d5fd0",
   "metadata": {},
   "source": [
    " ![Derivatives](data/images/image.png) ![Chain-Rule](data/images/image-1.png) ![Derivatives2](data/images/image-2.png) ![Derivatives3](data/images/image-3.png) ![Derivative4](data/images/image-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ff3204",
   "metadata": {},
   "source": [
    "An also simple example with graph from;\n",
    "[Image Link from Another Repo](https://github.com/rasbt/stat479-deep-learning-ss19/blob/master/L06_pytorch/code/pytorch-autograd.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e4309d",
   "metadata": {},
   "source": [
    "<img src=\"data/images/computationGraph.png\" alt=\"computation graph\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9be5f2b",
   "metadata": {},
   "source": [
    "### Jakobian Matris Calculation <br>\n",
    " The Jacobian matrix contains all first-order partial derivatives of a vector-valued function. For \n",
    "ùë¶=ùëì(ùë•)\n",
    "y=f(x), where ùë¶ and ùë•\n",
    "x are vectors, the Jacobian is a matrix where each element (i,j) is ‚àÇx <i>j</i>/‚àÇy <i>i</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "590712c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian matrix:\n",
      " tensor([[2., 1., 0.],\n",
      "        [0., 4., 0.],\n",
      "        [1., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Define input vector\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Define a vector-valued function\n",
    "def vector_func(x):\n",
    "    # y = [x0 * x1, x1 ** 2, x2 + x0]\n",
    "    return torch.stack([\n",
    "        x[0] * x[1],\n",
    "        x[1] ** 2,\n",
    "        x[2] + x[0]\n",
    "    ])\n",
    "\n",
    "# Calculate the Jacobian matrix\n",
    "jacobian = torch.autograd.functional.jacobian(vector_func, x)\n",
    "\n",
    "print(\"Jacobian matrix:\\n\", jacobian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947f346b",
   "metadata": {},
   "source": [
    "<img src=\"data/images/jacobian.png\" alt=\"Calculating Jacobian Matris\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed64065",
   "metadata": {},
   "source": [
    "### Custom Autograd Function (Forward-Backward) <br>\n",
    "Now that we have seen how PyTorch‚Äôs autograd automatically handles gradients and the chain rule, let‚Äôs take a step back and implement the same logic manually. By coding the forward and backward passes ourselves, we can clearly see what happens during backpropagation‚Äîand appreciate how much easier autograd makes our lives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b620a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 4.0, z: 925973.0, dz/dx: 1267110.0\n"
     ]
    }
   ],
   "source": [
    "class MyManualPolyTensor:\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = 3 * x**2 + 2 * x + 1\n",
    "        self.z = self.y**3 * 5 + 8\n",
    "        return self.z\n",
    "\n",
    "    def backward(self):\n",
    "        # dy/dx = 6x + 2\n",
    "        # dz/dy = 15 * y^2\n",
    "        dy_dx = 6 * self.x + 2\n",
    "        dz_dy = 15 * self.y ** 2\n",
    "        dz_dx = dz_dy * dy_dx\n",
    "        return dz_dx\n",
    "\n",
    "x = torch.tensor(4.0)  # requires_grad=False!\n",
    "\n",
    "m = MyManualPolyTensor()\n",
    "z = m.forward(x)\n",
    "grad = m.backward()\n",
    "print(f\"x: {x.item()}, z: {z.item()}, dz/dx: {grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3097d428",
   "metadata": {},
   "source": [
    "When we need custom behavior in the autograd system‚Äîsuch as defining our own forward and backward passes‚Äîwe subclass torch.autograd.Function and implement the forward and backward static methods ourselves.\n",
    "<ul>\n",
    "    <li>In the forward method, we compute and return the output(s) given the input(s), and we can save any tensors needed for the backward pass using ctx.save_for_backward.</li>\n",
    "    <li>In the backward method, we receive the gradient of the output with respect to some loss and must compute and return the gradient(s) with respect to each input.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bed84fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 3.0, y: 9.0, dy/dx: 6.0\n"
     ]
    }
   ],
   "source": [
    "class MySquare(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # In the forward pass, we receive a Tensor containing the input and return a Tensor containing the output.\n",
    "        ctx.save_for_backward(input)  # We save input for use in the backward pass.\n",
    "        return input ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # In the backward pass, we receive a Tensor containing the gradient of the loss with respect to the output,\n",
    "        # and we need to compute the gradient of the loss with respect to the input.\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = 2 * input * grad_output  # We apply the chain rule.\n",
    "        return grad_input\n",
    "\n",
    "# Now we use our custom autograd function.\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = MySquare.apply(x)\n",
    "y.backward()\n",
    "print(f\"x: {x.item()}, y: {y.item()}, dy/dx: {x.grad.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
